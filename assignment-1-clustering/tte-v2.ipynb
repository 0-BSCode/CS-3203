{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrialSequence(estimand='PP', data=     id  period  treatment  x1        x2  x3        x4  age     age_s  \\\n",
      "0     1       0          1   1  1.146148   0  0.734203   36  0.083333   \n",
      "1     1       1          1   1  0.002200   0  0.734203   37  0.166667   \n",
      "2     1       2          1   0 -0.481762   0  0.734203   38  0.250000   \n",
      "3     1       3          1   0  0.007872   0  0.734203   39  0.333333   \n",
      "4     1       4          1   1  0.216054   0  0.734203   40  0.416667   \n",
      "..   ..     ...        ...  ..       ...  ..       ...  ...       ...   \n",
      "720  99       3          0   0 -0.747906   1  0.575268   68  2.750000   \n",
      "721  99       4          0   0 -0.790056   1  0.575268   69  2.833333   \n",
      "722  99       5          1   1  0.387429   1  0.575268   70  2.916667   \n",
      "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000   \n",
      "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333   \n",
      "\n",
      "     outcome  censored  eligible  \n",
      "0          0         0         1  \n",
      "1          0         0         0  \n",
      "2          0         0         0  \n",
      "3          0         0         0  \n",
      "4          0         0         0  \n",
      "..       ...       ...       ...  \n",
      "720        0         0         0  \n",
      "721        0         0         0  \n",
      "722        0         0         0  \n",
      "723        0         0         0  \n",
      "724        1         0         0  \n",
      "\n",
      "[725 rows x 12 columns], id_col='id', period_col='period', treatment_col='treatment', outcome_col='outcome', eligible_col='eligible', switch_weights=    id  period    weight\n",
      "0    1       1  0.761800\n",
      "1    2       1  0.771315\n",
      "2    3       1  0.941859\n",
      "3    4       1  0.966432\n",
      "4    5       1  0.996027\n",
      "..  ..     ...       ...\n",
      "17  76      19  1.254239\n",
      "18  83      19  0.849307\n",
      "19  85      19  1.026541\n",
      "20  95      19  1.125288\n",
      "21  96      19  1.045674\n",
      "\n",
      "[636 rows x 3 columns], censor_weights=None, combined_weights=None, outcome_model=None, expansion=None, expansion_options=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan Sanchez\\AppData\\Local\\Temp\\ipykernel_4248\\4221744769.py:103: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_periods['switch_weight'].fillna(1.0, inplace=True)\n",
      "C:\\Users\\Bryan Sanchez\\AppData\\Local\\Temp\\ipykernel_4248\\4221744769.py:116: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_periods['censor_weight'].fillna(1.0, inplace=True)\n",
      "C:\\Users\\Bryan Sanchez\\AppData\\Local\\Temp\\ipykernel_4248\\4221744769.py:116: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_periods['censor_weight'].fillna(1.0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Models Summary:\n",
      "  Censor Weights: 725 rows\n",
      "  Combined Weights: 1780 rows\n",
      "Weight Models Summary:\n",
      "  Switch Weights: 636 rows\n",
      "  Censor Weights: 725 rows\n",
      "  Combined Weights: 1780 rows\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda\\envs\\cs-3202\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'weight'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 708\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;66;03m# Load expanded data and fit MSM\u001b[39;00m\n\u001b[0;32m    707\u001b[0m trial_itt \u001b[38;5;241m=\u001b[39m trial_itt\u001b[38;5;241m.\u001b[39mload_expanded_data(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m, p_control\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m--> 708\u001b[0m trial_itt \u001b[38;5;241m=\u001b[39m trial_itt\u001b[38;5;241m.\u001b[39mfit_msm(\n\u001b[0;32m    709\u001b[0m     weight_cols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    710\u001b[0m     modify_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m w: np\u001b[38;5;241m.\u001b[39mminimum(w, np\u001b[38;5;241m.\u001b[39mquantile(w, \u001b[38;5;241m0.99\u001b[39m))  \u001b[38;5;66;03m# Winsorization\u001b[39;00m\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m    714\u001b[0m prediction_data \u001b[38;5;241m=\u001b[39m outcome_data(trial_itt)\n",
      "Cell \u001b[1;32mIn[6], line 238\u001b[0m, in \u001b[0;36mTrialSequence.fit_msm\u001b[1;34m(self, weight_cols, modify_weights)\u001b[0m\n\u001b[0;32m    236\u001b[0m model_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m weight_cols:\n\u001b[1;32m--> 238\u001b[0m     model_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m model_data[col]\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Apply weight modification if provided\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modify_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\cs-3202\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\anaconda\\envs\\cs-3202\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'weight'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.duration.hazard_regression import PHReg\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Callable, Dict, Any, Union\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "# Dataclass to mimic the trial_sequence object in R\n",
    "@dataclass\n",
    "class TrialSequence:\n",
    "    estimand: str  # \"PP\" or \"ITT\"\n",
    "    data: Optional[pd.DataFrame] = None\n",
    "    id_col: Optional[str] = None\n",
    "    period_col: Optional[str] = None\n",
    "    treatment_col: Optional[str] = None\n",
    "    outcome_col: Optional[str] = None\n",
    "    eligible_col: Optional[str] = None\n",
    "    switch_weights: Optional[pd.DataFrame] = None\n",
    "    censor_weights: Optional[pd.DataFrame] = None\n",
    "    combined_weights: Optional[pd.DataFrame] = None\n",
    "    outcome_model: Optional[Any] = None\n",
    "    expansion: Optional[pd.DataFrame] = None\n",
    "    expansion_options: Optional[Dict] = None\n",
    "    \n",
    "    def set_data(self, data, id, period, treatment, outcome, eligible):\n",
    "        \"\"\"Set the data and column names for the trial sequence.\"\"\"\n",
    "        self.data = data\n",
    "        self.id_col = id\n",
    "        self.period_col = period\n",
    "        self.treatment_col = treatment\n",
    "        self.outcome_col = outcome\n",
    "        self.eligible_col = eligible\n",
    "        return self\n",
    "    \n",
    "    def set_switch_weight_model(self, numerator, denominator, model_fitter):\n",
    "        \"\"\"Set the switch weight model specifications.\"\"\"\n",
    "        # Convert R formula strings to lists of variable names\n",
    "        num_vars = self._formula_to_vars(numerator)\n",
    "        denom_vars = self._formula_to_vars(denominator)\n",
    "        \n",
    "        # Calculate switch weights\n",
    "        self.switch_weights = model_fitter.fit(\n",
    "            self.data, \n",
    "            self.treatment_col,\n",
    "            num_vars,\n",
    "            denom_vars,\n",
    "            self.id_col,\n",
    "            self.period_col\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_censor_weight_model(self, censor_event, numerator, denominator, pool_models, model_fitter):\n",
    "        \"\"\"Set the censoring weight model specifications.\"\"\"\n",
    "        # Convert R formula strings to lists of variable names\n",
    "        num_vars = self._formula_to_vars(numerator)\n",
    "        denom_vars = self._formula_to_vars(denominator)\n",
    "        \n",
    "        # Calculate censoring weights\n",
    "        censor_calculator = CensorWeightCalculator(\n",
    "            model_fitter=model_fitter,\n",
    "            censor_event=censor_event,\n",
    "            pool_models=pool_models\n",
    "        )\n",
    "        \n",
    "        self.censor_weights = censor_calculator.fit(\n",
    "            self.data,\n",
    "            num_vars,\n",
    "            denom_vars,\n",
    "            self.id_col,\n",
    "            self.period_col\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def calculate_weights(self):\n",
    "        \"\"\"Combine switch and censor weights.\"\"\"\n",
    "        if self.switch_weights is None and self.censor_weights is None:\n",
    "            raise ValueError(\"No weights have been calculated yet.\")\n",
    "        \n",
    "        # Create a base DataFrame with all patient-periods\n",
    "        all_periods = pd.DataFrame({\n",
    "            self.id_col: self.data[self.id_col].unique()\n",
    "        }).merge(\n",
    "            pd.DataFrame({self.period_col: self.data[self.period_col].unique()}),\n",
    "            how='cross'\n",
    "        )\n",
    "        \n",
    "        # Merge with switch weights if available\n",
    "        if self.switch_weights is not None:\n",
    "            all_periods = pd.merge(\n",
    "                all_periods,\n",
    "                self.switch_weights[[self.id_col, self.period_col, 'weight']],\n",
    "                on=[self.id_col, self.period_col],\n",
    "                how='left'\n",
    "            )\n",
    "            all_periods.rename(columns={'weight': 'switch_weight'}, inplace=True)\n",
    "            all_periods['switch_weight'].fillna(1.0, inplace=True)\n",
    "        else:\n",
    "            all_periods['switch_weight'] = 1.0\n",
    "        \n",
    "        # Merge with censor weights if available\n",
    "        if self.censor_weights is not None:\n",
    "            all_periods = pd.merge(\n",
    "                all_periods,\n",
    "                self.censor_weights[[self.id_col, self.period_col, 'weight']],\n",
    "                on=[self.id_col, self.period_col],\n",
    "                how='left'\n",
    "            )\n",
    "            all_periods.rename(columns={'weight': 'censor_weight'}, inplace=True)\n",
    "            all_periods['censor_weight'].fillna(1.0, inplace=True)\n",
    "        else:\n",
    "            all_periods['censor_weight'] = 1.0\n",
    "        \n",
    "        # Calculate combined weight\n",
    "        all_periods['weight'] = all_periods['switch_weight'] * all_periods['censor_weight']\n",
    "        \n",
    "        # Store combined weights\n",
    "        self.combined_weights = all_periods\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_outcome_model(self, adjustment_terms=None):\n",
    "        \"\"\"Set up the outcome model for survival analysis.\"\"\"\n",
    "        if adjustment_terms is None:\n",
    "            self.outcome_model = OutcomeModel()\n",
    "        else:\n",
    "            adj_vars = self._formula_to_vars(adjustment_terms)\n",
    "            self.outcome_model = OutcomeModel(adjustment_vars=adj_vars)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_expansion_options(self, output=None, chunk_size=500):\n",
    "        \"\"\"Set options for trial expansion.\"\"\"\n",
    "        self.expansion_options = {\n",
    "            'output_handler': output,\n",
    "            'chunk_size': chunk_size\n",
    "        }\n",
    "        return self\n",
    "    \n",
    "    def expand_trials(self):\n",
    "        \"\"\"Expand the trial data for analysis.\"\"\"\n",
    "        if self.expansion_options is None:\n",
    "            raise ValueError(\"Expansion options not set. Call set_expansion_options first.\")\n",
    "        \n",
    "        # Get unique individuals\n",
    "        individuals = self.data[self.id_col].unique()\n",
    "        \n",
    "        # Process in chunks\n",
    "        chunk_size = self.expansion_options['chunk_size']\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(individuals), chunk_size):\n",
    "            chunk_ids = individuals[i:i+chunk_size]\n",
    "            \n",
    "            # Filter data for current chunk\n",
    "            chunk_data = self.data[self.data[self.id_col].isin(chunk_ids)].copy()\n",
    "            \n",
    "            # Create expanded data\n",
    "            expanded = self._expand_individuals(chunk_data)\n",
    "            results.append(expanded)\n",
    "        \n",
    "        # Combine results\n",
    "        self.expansion = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _expand_individuals(self, data):\n",
    "        \"\"\"Create expanded data for a set of individuals.\"\"\"\n",
    "        expanded_data = []\n",
    "        \n",
    "        for id_val in data[self.id_col].unique():\n",
    "            # Get data for this individual\n",
    "            indiv_data = data[data[self.id_col] == id_val].sort_values(by=self.period_col)\n",
    "            \n",
    "            # For each period where the individual is eligible\n",
    "            for _, row in indiv_data[indiv_data[self.eligible_col] == 1].iterrows():\n",
    "                period = row[self.period_col]\n",
    "                treatment = row[self.treatment_col]\n",
    "                \n",
    "                # Create a trial record\n",
    "                trial_record = {\n",
    "                    self.id_col: id_val,\n",
    "                    'trial_period': period,\n",
    "                    'trial_arm': treatment,\n",
    "                    'original_' + self.period_col: period\n",
    "                }\n",
    "                \n",
    "                # Add original outcome and other columns\n",
    "                for col in [self.outcome_col, self.treatment_col]:\n",
    "                    trial_record['original_' + col] = row[col]\n",
    "                \n",
    "                expanded_data.append(trial_record)\n",
    "        \n",
    "        return pd.DataFrame(expanded_data)\n",
    "    \n",
    "    def load_expanded_data(self, seed=None, p_control=0.5):\n",
    "        \"\"\"Load expanded data and apply sampling weights.\"\"\"\n",
    "        if self.expansion is None:\n",
    "            raise ValueError(\"No expanded data available. Call expand_trials first.\")\n",
    "        \n",
    "        # Set random seed if provided\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Sample from expanded data\n",
    "        expanded_data = self.expansion.copy()\n",
    "        \n",
    "        # Calculate sampling weights based on treatment assignment\n",
    "        expanded_data['sample_weight'] = np.where(\n",
    "            expanded_data['trial_arm'] == 0,  # Assuming 0 is control\n",
    "            1.0 / p_control,\n",
    "            1.0 / (1.0 - p_control)\n",
    "        )\n",
    "        \n",
    "        # Store back to expansion\n",
    "        self.expansion = expanded_data\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_msm(self, weight_cols, modify_weights=None):\n",
    "        \"\"\"Fit a marginal structural model.\"\"\"\n",
    "        if self.expansion is None:\n",
    "            raise ValueError(\"No expanded data available. Call expand_trials and load_expanded_data first.\")\n",
    "        \n",
    "        # Prepare data for modeling\n",
    "        model_data = self.expansion.copy()\n",
    "        \n",
    "        # Combine weights\n",
    "        model_data['combined_weight'] = 1.0\n",
    "        for col in weight_cols:\n",
    "            model_data['combined_weight'] *= model_data[col]\n",
    "        \n",
    "        # Apply weight modification if provided\n",
    "        if modify_weights is not None:\n",
    "            model_data['combined_weight'] = modify_weights(model_data['combined_weight'])\n",
    "        \n",
    "        # Fit outcome model\n",
    "        if self.outcome_model is None:\n",
    "            self.set_outcome_model()\n",
    "        \n",
    "        self.outcome_model.fit(\n",
    "            data=model_data,\n",
    "            id_col=self.id_col,\n",
    "            time_col='followup_time',  # Assuming this is in the expanded data\n",
    "            event_col=self.outcome_col,\n",
    "            treatment_col='trial_arm',\n",
    "            weight_col='combined_weight'\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, newdata, predict_times, type=\"survival\"):\n",
    "        \"\"\"Predict outcomes based on the fitted model.\"\"\"\n",
    "        if self.outcome_model is None or not self.outcome_model.is_fitted():\n",
    "            raise ValueError(\"Outcome model not fitted. Call fit_msm first.\")\n",
    "        \n",
    "        # Prepare prediction data\n",
    "        pred_data = newdata.copy()\n",
    "        \n",
    "        # Make predictions for each treatment arm\n",
    "        results = {}\n",
    "        for arm in [0, 1]:  # Assuming binary treatment (0=control, 1=treatment)\n",
    "            pred_data['trial_arm'] = arm\n",
    "            surv_curves = self.outcome_model.predict(pred_data, predict_times)\n",
    "            results[f'arm_{arm}'] = surv_curves\n",
    "        \n",
    "        # Calculate difference (treatment effect)\n",
    "        diff_data = pd.DataFrame({\n",
    "            'followup_time': predict_times,\n",
    "            'survival_diff': results['arm_1']['survival'] - results['arm_0']['survival'],\n",
    "            '2.5%': results['arm_1']['lower'] - results['arm_0']['upper'],  # Conservative CI\n",
    "            '97.5%': results['arm_1']['upper'] - results['arm_0']['lower']  # Conservative CI\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'arm_0': results['arm_0'],\n",
    "            'arm_1': results['arm_1'],\n",
    "            'difference': diff_data\n",
    "        }\n",
    "    \n",
    "    def _formula_to_vars(self, formula):\n",
    "        \"\"\"Convert an R-style formula to a list of variable names.\"\"\"\n",
    "        if isinstance(formula, str):\n",
    "            # Remove ~ and split by +\n",
    "            parts = formula.replace(\"~\", \"\").split(\"+\")\n",
    "            return [part.strip() for part in parts]\n",
    "        else:\n",
    "            # For our example, we'll handle the simpler case where the formula is a string like \"~ age + x1 + x3\"\n",
    "            # For the case where formula is passed as a tilde object in R, we just extract the string after the tilde\n",
    "            formula_str = formula.replace(\"~\", \"\")\n",
    "            parts = formula_str.split(\"+\")\n",
    "            return [part.strip() for part in parts]\n",
    "\n",
    "\n",
    "class StatsGlmLogit:\n",
    "    def __init__(self, save_path=None):\n",
    "        self.save_path = save_path\n",
    "        if save_path and not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "    \n",
    "    def fit(self, data, treatment_col, numerator_vars, denominator_vars, id_col, period_col):\n",
    "        \"\"\"Fit logistic regression models and calculate stabilized weights.\"\"\"\n",
    "        # Get periods where treatment can switch\n",
    "        periods = sorted(data[period_col].unique())\n",
    "        \n",
    "        # Initialize DataFrame to store weights\n",
    "        weights_df = pd.DataFrame()\n",
    "        \n",
    "        for period in periods[1:]:  # Skip the first period as there's no prior treatment to switch from\n",
    "            # Get data for current period\n",
    "            period_data = data[data[period_col] == period].copy()\n",
    "            \n",
    "            if len(period_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get previous period for each individual\n",
    "            prev_period = periods[periods.index(period) - 1]\n",
    "            prev_data = data[data[period_col] == prev_period].copy()\n",
    "            \n",
    "            # Merge current with previous period data\n",
    "            merged_data = pd.merge(\n",
    "                period_data,\n",
    "                prev_data[[id_col, treatment_col]],\n",
    "                on=id_col,\n",
    "                suffixes=('', '_prev')\n",
    "            )\n",
    "            \n",
    "            # Identify individuals who switched treatment\n",
    "            merged_data['switched'] = (merged_data[treatment_col] != merged_data[f\"{treatment_col}_prev\"]).astype(int)\n",
    "            \n",
    "            # Fit numerator model (simpler model)\n",
    "            X_num = sm.add_constant(merged_data[numerator_vars])\n",
    "            num_model = sm.Logit(merged_data['switched'], X_num).fit(disp=0)\n",
    "            \n",
    "            # Save model if requested\n",
    "            if self.save_path:\n",
    "                with open(os.path.join(self.save_path, f\"num_model_period_{period}.pkl\"), 'wb') as f:\n",
    "                    pickle.dump(num_model, f)\n",
    "            \n",
    "            # Fit denominator model (full model)\n",
    "            X_denom = sm.add_constant(merged_data[denominator_vars])\n",
    "            denom_model = sm.Logit(merged_data['switched'], X_denom).fit(disp=0)\n",
    "            \n",
    "            # Save model if requested\n",
    "            if self.save_path:\n",
    "                with open(os.path.join(self.save_path, f\"denom_model_period_{period}.pkl\"), 'wb') as f:\n",
    "                    pickle.dump(denom_model, f)\n",
    "            \n",
    "            # Calculate predicted probabilities\n",
    "            num_probs = num_model.predict(X_num)\n",
    "            denom_probs = denom_model.predict(X_denom)\n",
    "            \n",
    "            # Calculate stabilized weights\n",
    "            merged_data['weight'] = num_probs / denom_probs\n",
    "            merged_data['weight'] = merged_data['weight'].fillna(1.0)  # Handle division by zero\n",
    "            \n",
    "            # Add to weights DataFrame\n",
    "            weights_df = pd.concat([weights_df, merged_data[[id_col, period_col, 'weight']]])\n",
    "        \n",
    "        return weights_df\n",
    "\n",
    "\n",
    "class CensorWeightCalculator:\n",
    "    def __init__(self, model_fitter, censor_event, pool_models=\"none\"):\n",
    "        self.model_fitter = model_fitter\n",
    "        self.censor_event = censor_event\n",
    "        self.pool_models = pool_models  # \"none\", \"numerator\", or \"denominator\"\n",
    "    \n",
    "    def fit(self, data, numerator_vars, denominator_vars, id_col, period_col):\n",
    "        \"\"\"Calculate censoring weights.\"\"\"\n",
    "        # Create a copy of the data\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Create pooled models if required\n",
    "        if self.pool_models in [\"numerator\", \"both\"]:\n",
    "            pooled_num_model = self._fit_pooled_model(data_copy, numerator_vars)\n",
    "        \n",
    "        if self.pool_models in [\"denominator\", \"both\"]:\n",
    "            pooled_denom_model = self._fit_pooled_model(data_copy, denominator_vars)\n",
    "        \n",
    "        # Get periods\n",
    "        periods = sorted(data_copy[period_col].unique())\n",
    "        \n",
    "        # Initialize DataFrame to store weights\n",
    "        weights_df = pd.DataFrame()\n",
    "        \n",
    "        for period in periods:\n",
    "            # Get data for current period\n",
    "            period_data = data_copy[data_copy[period_col] == period].copy()\n",
    "            \n",
    "            if len(period_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Define censoring indicator\n",
    "            period_data['is_censored'] = (period_data[self.censor_event] == 1).astype(int)\n",
    "            \n",
    "            # Check if we have variation in the outcome\n",
    "            if period_data['is_censored'].nunique() <= 1:\n",
    "                # No variation, assign weight of 1.0\n",
    "                period_data['weight'] = 1.0\n",
    "            else:\n",
    "                try:\n",
    "                    # Fit period-specific models or use pooled models\n",
    "                    if self.pool_models != \"numerator\":\n",
    "                        # Fit numerator model for this period with regularization\n",
    "                        X_num = sm.add_constant(period_data[numerator_vars])\n",
    "                        num_model = sm.Logit(period_data['is_censored'], X_num).fit_regularized(\n",
    "                            alpha=0.01, disp=0\n",
    "                        )\n",
    "                        num_probs = num_model.predict(X_num)\n",
    "                    else:\n",
    "                        # Use pooled numerator model\n",
    "                        X_num = sm.add_constant(period_data[numerator_vars])\n",
    "                        num_probs = pooled_num_model.predict(X_num)\n",
    "                    \n",
    "                    if self.pool_models != \"denominator\":\n",
    "                        # Fit denominator model for this period with regularization\n",
    "                        X_denom = sm.add_constant(period_data[denominator_vars])\n",
    "                        denom_model = sm.Logit(period_data['is_censored'], X_denom).fit_regularized(\n",
    "                            alpha=0.01, disp=0\n",
    "                        )\n",
    "                        denom_probs = denom_model.predict(X_denom)\n",
    "                    else:\n",
    "                        # Use pooled denominator model\n",
    "                        X_denom = sm.add_constant(period_data[denominator_vars])\n",
    "                        denom_probs = pooled_denom_model.predict(X_denom)\n",
    "                    \n",
    "                    # Ensure probabilities are not exactly 0 or 1\n",
    "                    num_probs = np.clip(num_probs, 0.001, 0.999)\n",
    "                    denom_probs = np.clip(denom_probs, 0.001, 0.999)\n",
    "                    \n",
    "                    # Calculate stabilized weights\n",
    "                    period_data['weight'] = (1 - num_probs) / (1 - denom_probs)\n",
    "                    \n",
    "                except (np.linalg.LinAlgError, ValueError) as e:\n",
    "                    print(f\"Warning: Model fitting failed for period {period}. Setting weights to 1.0. Error: {e}\")\n",
    "                    period_data['weight'] = 1.0\n",
    "                \n",
    "            # Handle any remaining NaNs or infinities\n",
    "            period_data['weight'] = period_data['weight'].fillna(1.0)\n",
    "            period_data.loc[np.isinf(period_data['weight']), 'weight'] = 1.0\n",
    "            \n",
    "            # Trim extreme weights\n",
    "            q99 = np.percentile(period_data['weight'], 99)\n",
    "            period_data.loc[period_data['weight'] > q99, 'weight'] = q99\n",
    "            \n",
    "            # Add to weights DataFrame\n",
    "            weights_df = pd.concat([weights_df, period_data[[id_col, period_col, 'weight']]])\n",
    "        \n",
    "        return weights_df\n",
    "\n",
    "    def _fit_pooled_model(self, data, vars_list):\n",
    "        \"\"\"Fit a pooled model across all periods.\"\"\"\n",
    "        # Create censoring indicator\n",
    "        data['is_censored'] = (data[self.censor_event] == 1).astype(int)\n",
    "        \n",
    "        # Check if we have variation in the outcome\n",
    "        if data['is_censored'].nunique() <= 1:\n",
    "            # Return a dummy model that always predicts the constant\n",
    "            constant_prob = data['is_censored'].mean()\n",
    "            class DummyModel:\n",
    "                def predict(self, X):\n",
    "                    return np.ones(len(X)) * constant_prob\n",
    "            return DummyModel()\n",
    "        \n",
    "        try:\n",
    "            # Fit model with regularization\n",
    "            X = sm.add_constant(data[vars_list])\n",
    "            model = sm.Logit(data['is_censored'], X).fit_regularized(alpha=0.01, disp=0)\n",
    "            return model\n",
    "        except (np.linalg.LinAlgError, ValueError) as e:\n",
    "            print(f\"Warning: Pooled model fitting failed. Creating dummy model. Error: {e}\")\n",
    "            # Return a dummy model\n",
    "            constant_prob = data['is_censored'].mean()\n",
    "            class DummyModel:\n",
    "                def predict(self, X):\n",
    "                    return np.ones(len(X)) * constant_prob\n",
    "            return DummyModel()\n",
    "\n",
    "\n",
    "class OutcomeModel:\n",
    "    def __init__(self, adjustment_vars=None):\n",
    "        self.adjustment_vars = adjustment_vars\n",
    "        self.fitted_model = None\n",
    "        self.model_info = None\n",
    "    \n",
    "    def fit(self, data, id_col, time_col, event_col, treatment_col, weight_col):\n",
    "        \"\"\"Fit a proportional hazards model.\"\"\"\n",
    "        model_data = data.copy()\n",
    "        \n",
    "        # Prepare formula\n",
    "        if self.adjustment_vars:\n",
    "            formula = f\"{time_col} ~ {treatment_col} + \" + \" + \".join(self.adjustment_vars)\n",
    "        else:\n",
    "            formula = f\"{time_col} ~ {treatment_col}\"\n",
    "        \n",
    "        # Fit Cox PH model\n",
    "        model = PHReg.from_formula(\n",
    "            formula,\n",
    "            data=model_data,\n",
    "            status=model_data[event_col],\n",
    "            weights=model_data[weight_col]\n",
    "        )\n",
    "        \n",
    "        result = model.fit()\n",
    "        \n",
    "        # Store fitted model\n",
    "        self.fitted_model = result\n",
    "        \n",
    "        # Store model info\n",
    "        self.model_info = {\n",
    "            'model': model,\n",
    "            'vcov': result.cov_params(),\n",
    "            'formula': formula\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def is_fitted(self):\n",
    "        \"\"\"Check if model is fitted.\"\"\"\n",
    "        return self.fitted_model is not None\n",
    "    \n",
    "    def predict(self, data, times):\n",
    "        \"\"\"Predict survival probabilities.\"\"\"\n",
    "        if not self.is_fitted():\n",
    "            raise ValueError(\"Model not fitted yet.\")\n",
    "        \n",
    "        # Prepare prediction data\n",
    "        pred_data = data.copy()\n",
    "        \n",
    "        # Get baseline survival\n",
    "        baseline_surv = self._estimate_baseline_survival(times)\n",
    "        \n",
    "        # Get linear predictor for new data\n",
    "        lp = self._calculate_linear_predictor(pred_data)\n",
    "        \n",
    "        # Calculate survival probabilities\n",
    "        survival = np.power(baseline_surv, np.exp(lp))\n",
    "        \n",
    "        # Calculate confidence intervals (simplified)\n",
    "        # For a proper implementation, we would need to calculate the variance of the survival estimates\n",
    "        ci_width = 1.96 * 0.1 * survival  # Simplified CI\n",
    "        \n",
    "        return {\n",
    "            'times': times,\n",
    "            'survival': survival,\n",
    "            'lower': np.maximum(0, survival - ci_width),\n",
    "            'upper': np.minimum(1, survival + ci_width)\n",
    "        }\n",
    "    \n",
    "    def _estimate_baseline_survival(self, times):\n",
    "        \"\"\"Estimate baseline survival function.\"\"\"\n",
    "        # This is a simplified implementation\n",
    "        # A proper implementation would estimate the baseline hazard from the data\n",
    "        \n",
    "        # For simplicity, we'll use an exponential model\n",
    "        # In real implementation, this would be based on the fitted model's baseline hazard\n",
    "        lambda_hat = 0.1  # Placeholder hazard rate\n",
    "        return np.exp(-lambda_hat * np.array(times))\n",
    "    \n",
    "    def _calculate_linear_predictor(self, data):\n",
    "        \"\"\"Calculate linear predictor for new data.\"\"\"\n",
    "        # Extract coefficients from fitted model\n",
    "        coefs = self.fitted_model.params\n",
    "        \n",
    "        # Prepare design matrix\n",
    "        X = np.ones((len(data), len(coefs)))\n",
    "        X[:, 0] = data['trial_arm']  # Treatment indicator\n",
    "        \n",
    "        # Add adjustment variables if specified\n",
    "        if self.adjustment_vars:\n",
    "            for i, var in enumerate(self.adjustment_vars):\n",
    "                X[:, i+1] = data[var]\n",
    "        \n",
    "        # Calculate linear predictor\n",
    "        return X @ coefs\n",
    "\n",
    "\n",
    "def trial_sequence(estimand):\n",
    "    \"\"\"Create a new trial sequence object.\"\"\"\n",
    "    return TrialSequence(estimand=estimand)\n",
    "\n",
    "\n",
    "def stats_glm_logit(save_path=None):\n",
    "    \"\"\"Create a logistic regression model fitter.\"\"\"\n",
    "    return StatsGlmLogit(save_path=save_path)\n",
    "\n",
    "\n",
    "def save_to_datatable():\n",
    "    \"\"\"Handler for saving expanded data to a data table.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    def handler(data):\n",
    "        return data\n",
    "    return handler\n",
    "\n",
    "\n",
    "def outcome_data(trial):\n",
    "    \"\"\"Extract outcome data from a trial sequence.\"\"\"\n",
    "    if trial.expansion is not None:\n",
    "        return trial.expansion\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def show_weight_models(trial):\n",
    "    \"\"\"Display information about weight models.\"\"\"\n",
    "    print(\"Weight Models Summary:\")\n",
    "    if hasattr(trial, 'switch_weights') and trial.switch_weights is not None:\n",
    "        print(f\"  Switch Weights: {len(trial.switch_weights)} rows\")\n",
    "    \n",
    "    if hasattr(trial, 'censor_weights') and trial.censor_weights is not None:\n",
    "        print(f\"  Censor Weights: {len(trial.censor_weights)} rows\")\n",
    "    \n",
    "    if hasattr(trial, 'combined_weights') and trial.combined_weights is not None:\n",
    "        print(f\"  Combined Weights: {len(trial.combined_weights)} rows\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    data_censored = pd.read_csv(\"data/data_censored.csv\")\n",
    "    \n",
    "    # Create trial sequence objects\n",
    "    trial_pp = trial_sequence(estimand=\"PP\")  # Per-protocol\n",
    "    trial_itt = trial_sequence(estimand=\"ITT\")  # Intention-to-treat\n",
    "    \n",
    "    # Create directories\n",
    "    trial_pp_dir = os.path.join(tempfile.gettempdir(), \"trial_pp\")\n",
    "    os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "    \n",
    "    trial_itt_dir = os.path.join(tempfile.gettempdir(), \"trial_itt\")\n",
    "    os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "    \n",
    "    # Set data for both trial sequences\n",
    "    trial_pp = trial_pp.set_data(\n",
    "        data=data_censored,\n",
    "        id=\"id\",\n",
    "        period=\"period\",\n",
    "        treatment=\"treatment\",\n",
    "        outcome=\"outcome\",\n",
    "        eligible=\"eligible\"\n",
    "    )\n",
    "    \n",
    "    trial_itt = trial_itt.set_data(\n",
    "        data=data_censored,\n",
    "        id=\"id\",\n",
    "        period=\"period\",\n",
    "        treatment=\"treatment\",\n",
    "        outcome=\"outcome\",\n",
    "        eligible=\"eligible\"\n",
    "    )\n",
    "    \n",
    "    # Set switch weight models\n",
    "    trial_pp = trial_pp.set_switch_weight_model(\n",
    "        numerator=\"~ age\",\n",
    "        denominator=\"~ age + x1 + x3\",\n",
    "        model_fitter=stats_glm_logit(save_path=os.path.join(trial_pp_dir, \"switch_models\"))\n",
    "    )\n",
    "    \n",
    "    print(trial_pp)\n",
    "    # Set censor weight models\n",
    "    trial_pp = trial_pp.set_censor_weight_model(\n",
    "        censor_event=\"censored\",\n",
    "        numerator=\"~ x2\",\n",
    "        denominator=\"~ x2 + x1\",\n",
    "        pool_models=\"none\",\n",
    "        model_fitter=stats_glm_logit(save_path=os.path.join(trial_pp_dir, \"switch_models\"))\n",
    "    )\n",
    "    \n",
    "    trial_itt = trial_itt.set_censor_weight_model(\n",
    "        censor_event=\"censored\",\n",
    "        numerator=\"~ x2\",\n",
    "        denominator=\"~ x2 + x1\",\n",
    "        pool_models=\"numerator\",\n",
    "        model_fitter=stats_glm_logit(save_path=os.path.join(trial_itt_dir, \"switch_models\"))\n",
    "    )\n",
    "    \n",
    "    # Calculate weights\n",
    "    trial_pp = trial_pp.calculate_weights()\n",
    "    trial_itt = trial_itt.calculate_weights()\n",
    "    \n",
    "    # Show weight models\n",
    "    show_weight_models(trial_itt)\n",
    "    show_weight_models(trial_pp)\n",
    "    \n",
    "    # Set outcome models\n",
    "    trial_pp = trial_pp.set_outcome_model()\n",
    "    trial_itt = trial_itt.set_outcome_model(adjustment_terms=\"~ x2\")\n",
    "    \n",
    "    # Set expansion options\n",
    "    trial_pp = trial_pp.set_expansion_options(\n",
    "        output=save_to_datatable(),\n",
    "        chunk_size=500\n",
    "    )\n",
    "    \n",
    "    trial_itt = trial_itt.set_expansion_options(\n",
    "        output=save_to_datatable(),\n",
    "        chunk_size=500\n",
    "    )\n",
    "    \n",
    "    # Expand trials\n",
    "    trial_pp = trial_pp.expand_trials()\n",
    "    trial_itt = trial_itt.expand_trials()\n",
    "    \n",
    "    # Load expanded data and fit MSM\n",
    "    trial_itt = trial_itt.load_expanded_data(seed=1234, p_control=0.5)\n",
    "    trial_itt = trial_itt.fit_msm(\n",
    "        weight_cols=[\"weight\", \"sample_weight\"],\n",
    "        modify_weights=lambda w: np.minimum(w, np.quantile(w, 0.99))  # Winsorization\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    prediction_data = outcome_data(trial_itt)\n",
    "    prediction_data = prediction_data[prediction_data['trial_period'] == 1]\n",
    "    \n",
    "    preds = trial_itt.predict(\n",
    "        newdata=prediction_data,\n",
    "        predict_times=list(range(11)),  # 0 to 10\n",
    "        type=\"survival\"\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(preds['difference']['followup_time'], preds['difference']['survival_diff'])\n",
    "    plt.plot(preds['difference']['followup_time'], preds['difference']['2.5%'], 'r--')\n",
    "    plt.plot(preds['difference']['followup_time'], preds['difference']['97.5%'], 'r--')\n",
    "    plt.xlabel('Follow up')\n",
    "    plt.ylabel('Survival difference')\n",
    "    plt.title('Treatment Effect on Survival')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('survival_difference.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrialSequence(estimand='PP', data=     id  period  treatment  x1        x2  x3        x4  age     age_s  \\\n",
      "0     1       0          1   1  1.146148   0  0.734203   36  0.083333   \n",
      "1     1       1          1   1  0.002200   0  0.734203   37  0.166667   \n",
      "2     1       2          1   0 -0.481762   0  0.734203   38  0.250000   \n",
      "3     1       3          1   0  0.007872   0  0.734203   39  0.333333   \n",
      "4     1       4          1   1  0.216054   0  0.734203   40  0.416667   \n",
      "..   ..     ...        ...  ..       ...  ..       ...  ...       ...   \n",
      "720  99       3          0   0 -0.747906   1  0.575268   68  2.750000   \n",
      "721  99       4          0   0 -0.790056   1  0.575268   69  2.833333   \n",
      "722  99       5          1   1  0.387429   1  0.575268   70  2.916667   \n",
      "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000   \n",
      "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333   \n",
      "\n",
      "     outcome  censored  eligible  \n",
      "0          0         0         1  \n",
      "1          0         0         0  \n",
      "2          0         0         0  \n",
      "3          0         0         0  \n",
      "4          0         0         0  \n",
      "..       ...       ...       ...  \n",
      "720        0         0         0  \n",
      "721        0         0         0  \n",
      "722        0         0         0  \n",
      "723        0         0         0  \n",
      "724        1         0         0  \n",
      "\n",
      "[725 rows x 12 columns], id_col='id', period_col='period', treatment_col='treatment', outcome_col='outcome', eligible_col='eligible', switch_weights=    id  period    weight\n",
      "0    1       1  0.761800\n",
      "1    2       1  0.771315\n",
      "2    3       1  0.941859\n",
      "3    4       1  0.966432\n",
      "4    5       1  0.996027\n",
      "..  ..     ...       ...\n",
      "17  76      19  1.254239\n",
      "18  83      19  0.849307\n",
      "19  85      19  1.026541\n",
      "20  95      19  1.125288\n",
      "21  96      19  1.045674\n",
      "\n",
      "[636 rows x 3 columns], censor_weights=None, combined_weights=None, outcome_model=None, expansion=None, expansion_options=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan Sanchez\\AppData\\Local\\Temp\\ipykernel_4248\\2138355202.py:103: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_periods['switch_weight'].fillna(1.0, inplace=True)\n",
      "C:\\Users\\Bryan Sanchez\\AppData\\Local\\Temp\\ipykernel_4248\\2138355202.py:116: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_periods['censor_weight'].fillna(1.0, inplace=True)\n",
      "C:\\Users\\Bryan Sanchez\\AppData\\Local\\Temp\\ipykernel_4248\\2138355202.py:116: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_periods['censor_weight'].fillna(1.0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Models Summary:\n",
      "  Censor Weights: 725 rows\n",
      "  Combined Weights: 1780 rows\n",
      "Weight Models Summary:\n",
      "  Switch Weights: 636 rows\n",
      "  Censor Weights: 725 rows\n",
      "  Combined Weights: 1780 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan Sanchez\\AppData\\Local\\Temp\\ipykernel_4248\\2138355202.py:771: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prediction_data['const'] = -1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.duration.hazard_regression import PHReg\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Callable, Dict, Any, Union\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "# Dataclass to mimic the trial_sequence object in R\n",
    "@dataclass\n",
    "class TrialSequence:\n",
    "    estimand: str  # \"PP\" or \"ITT\"\n",
    "    data: Optional[pd.DataFrame] = None\n",
    "    id_col: Optional[str] = None\n",
    "    period_col: Optional[str] = None\n",
    "    treatment_col: Optional[str] = None\n",
    "    outcome_col: Optional[str] = None\n",
    "    eligible_col: Optional[str] = None\n",
    "    switch_weights: Optional[pd.DataFrame] = None\n",
    "    censor_weights: Optional[pd.DataFrame] = None\n",
    "    combined_weights: Optional[pd.DataFrame] = None\n",
    "    outcome_model: Optional[Any] = None\n",
    "    expansion: Optional[pd.DataFrame] = None\n",
    "    expansion_options: Optional[Dict] = None\n",
    "    \n",
    "    def set_data(self, data, id, period, treatment, outcome, eligible):\n",
    "        \"\"\"Set the data and column names for the trial sequence.\"\"\"\n",
    "        self.data = data\n",
    "        self.id_col = id\n",
    "        self.period_col = period\n",
    "        self.treatment_col = treatment\n",
    "        self.outcome_col = outcome\n",
    "        self.eligible_col = eligible\n",
    "        return self\n",
    "    \n",
    "    def set_switch_weight_model(self, numerator, denominator, model_fitter):\n",
    "        \"\"\"Set the switch weight model specifications.\"\"\"\n",
    "        # Convert R formula strings to lists of variable names\n",
    "        num_vars = self._formula_to_vars(numerator)\n",
    "        denom_vars = self._formula_to_vars(denominator)\n",
    "        \n",
    "        # Calculate switch weights\n",
    "        self.switch_weights = model_fitter.fit(\n",
    "            self.data, \n",
    "            self.treatment_col,\n",
    "            num_vars,\n",
    "            denom_vars,\n",
    "            self.id_col,\n",
    "            self.period_col\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_censor_weight_model(self, censor_event, numerator, denominator, pool_models, model_fitter):\n",
    "        \"\"\"Set the censoring weight model specifications.\"\"\"\n",
    "        # Convert R formula strings to lists of variable names\n",
    "        num_vars = self._formula_to_vars(numerator)\n",
    "        denom_vars = self._formula_to_vars(denominator)\n",
    "        \n",
    "        # Calculate censoring weights\n",
    "        censor_calculator = CensorWeightCalculator(\n",
    "            model_fitter=model_fitter,\n",
    "            censor_event=censor_event,\n",
    "            pool_models=pool_models\n",
    "        )\n",
    "        \n",
    "        self.censor_weights = censor_calculator.fit(\n",
    "            self.data,\n",
    "            num_vars,\n",
    "            denom_vars,\n",
    "            self.id_col,\n",
    "            self.period_col\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def calculate_weights(self):\n",
    "        \"\"\"Combine switch and censor weights.\"\"\"\n",
    "        if self.switch_weights is None and self.censor_weights is None:\n",
    "            raise ValueError(\"No weights have been calculated yet.\")\n",
    "        \n",
    "        # Create a base DataFrame with all patient-periods\n",
    "        all_periods = pd.DataFrame({\n",
    "            self.id_col: self.data[self.id_col].unique()\n",
    "        }).merge(\n",
    "            pd.DataFrame({self.period_col: self.data[self.period_col].unique()}),\n",
    "            how='cross'\n",
    "        )\n",
    "        \n",
    "        # Merge with switch weights if available\n",
    "        if self.switch_weights is not None:\n",
    "            all_periods = pd.merge(\n",
    "                all_periods,\n",
    "                self.switch_weights[[self.id_col, self.period_col, 'weight']],\n",
    "                on=[self.id_col, self.period_col],\n",
    "                how='left'\n",
    "            )\n",
    "            all_periods.rename(columns={'weight': 'switch_weight'}, inplace=True)\n",
    "            all_periods['switch_weight'].fillna(1.0, inplace=True)\n",
    "        else:\n",
    "            all_periods['switch_weight'] = 1.0\n",
    "        \n",
    "        # Merge with censor weights if available\n",
    "        if self.censor_weights is not None:\n",
    "            all_periods = pd.merge(\n",
    "                all_periods,\n",
    "                self.censor_weights[[self.id_col, self.period_col, 'weight']],\n",
    "                on=[self.id_col, self.period_col],\n",
    "                how='left'\n",
    "            )\n",
    "            all_periods.rename(columns={'weight': 'censor_weight'}, inplace=True)\n",
    "            all_periods['censor_weight'].fillna(1.0, inplace=True)\n",
    "        else:\n",
    "            all_periods['censor_weight'] = 1.0\n",
    "        \n",
    "        # Calculate combined weight\n",
    "        all_periods['weight'] = all_periods['switch_weight'] * all_periods['censor_weight']\n",
    "        \n",
    "        # Store combined weights\n",
    "        self.combined_weights = all_periods\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_outcome_model(self, adjustment_terms=None):\n",
    "        \"\"\"Set up the outcome model for survival analysis.\"\"\"\n",
    "        if adjustment_terms is None:\n",
    "            self.outcome_model = OutcomeModel()\n",
    "        else:\n",
    "            adj_vars = self._formula_to_vars(adjustment_terms)\n",
    "            self.outcome_model = OutcomeModel(adjustment_vars=adj_vars)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_expansion_options(self, output=None, chunk_size=500):\n",
    "        \"\"\"Set options for trial expansion.\"\"\"\n",
    "        self.expansion_options = {\n",
    "            'output_handler': output,\n",
    "            'chunk_size': chunk_size\n",
    "        }\n",
    "        return self\n",
    "    \n",
    "    def expand_trials(self):\n",
    "        \"\"\"Expand the trial data for analysis.\"\"\"\n",
    "        if self.expansion_options is None:\n",
    "            raise ValueError(\"Expansion options not set. Call set_expansion_options first.\")\n",
    "        \n",
    "        # Get unique individuals\n",
    "        individuals = self.data[self.id_col].unique()\n",
    "        \n",
    "        # Process in chunks\n",
    "        chunk_size = self.expansion_options['chunk_size']\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(individuals), chunk_size):\n",
    "            chunk_ids = individuals[i:i+chunk_size]\n",
    "            \n",
    "            # Filter data for current chunk\n",
    "            chunk_data = self.data[self.data[self.id_col].isin(chunk_ids)].copy()\n",
    "            \n",
    "            # Create expanded data\n",
    "            expanded = self._expand_individuals(chunk_data)\n",
    "            results.append(expanded)\n",
    "        \n",
    "        # Combine results\n",
    "        self.expansion = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _expand_individuals(self, data):\n",
    "        \"\"\"Create expanded data for a set of individuals with survival times.\"\"\"\n",
    "        expanded_data = []\n",
    "        \n",
    "        for id_val in data[self.id_col].unique():\n",
    "            # Get individual's data, sorted by period\n",
    "            indiv_data = data[data[self.id_col] == id_val].sort_values(by=self.period_col)\n",
    "            \n",
    "            # Find event time: first period where outcome == 1 or censored == 1\n",
    "            event_rows = indiv_data[(indiv_data[self.outcome_col] == 1) | (indiv_data['censored'] == 1)]\n",
    "            if not event_rows.empty:\n",
    "                event_time = event_rows.iloc[0][self.period_col]\n",
    "                event_status = event_rows.iloc[0][self.outcome_col]\n",
    "            else:\n",
    "                # No event or censoring; assume censored at last period\n",
    "                event_time = indiv_data[self.period_col].max()\n",
    "                event_status = 0\n",
    "            \n",
    "            # Eligible start periods are before or at event_time\n",
    "            eligible_data = indiv_data[(indiv_data[self.eligible_col] == 1) & \n",
    "                                    (indiv_data[self.period_col] <= event_time)]\n",
    "            \n",
    "            for _, row in eligible_data.iterrows():\n",
    "                start_period = row[self.period_col]\n",
    "                trial_arm = row[self.treatment_col]\n",
    "                survival_time = event_time - start_period\n",
    "                event = event_status\n",
    "                \n",
    "                # Create trial record\n",
    "                record = {\n",
    "                    self.id_col: id_val,\n",
    "                    'trial_period': start_period,\n",
    "                    'trial_arm': trial_arm,\n",
    "                    'survival_time': survival_time,\n",
    "                    'event': event,\n",
    "                }\n",
    "                \n",
    "                # Include baseline covariates from start period\n",
    "                for col in self.data.columns:\n",
    "                    if col not in [self.id_col, self.period_col, self.treatment_col, \n",
    "                                self.outcome_col, 'censored', self.eligible_col]:\n",
    "                        record[col] = row[col]\n",
    "                \n",
    "                # Include weight for this trial from combined_weights\n",
    "                if self.combined_weights is not None:\n",
    "                    weight_row = self.combined_weights[\n",
    "                        (self.combined_weights[self.id_col] == id_val) & \n",
    "                        (self.combined_weights[self.period_col] == start_period)\n",
    "                    ]\n",
    "                    record['weight'] = weight_row['weight'].values[0] if not weight_row.empty else 1.0\n",
    "                else:\n",
    "                    record['weight'] = 1.0\n",
    "                \n",
    "                expanded_data.append(record)\n",
    "        \n",
    "        return pd.DataFrame(expanded_data)\n",
    "    \n",
    "    def load_expanded_data(self, seed=None, p_control=0.5):\n",
    "        \"\"\"Load expanded data and apply sampling weights.\"\"\"\n",
    "        if self.expansion is None:\n",
    "            raise ValueError(\"No expanded data available. Call expand_trials first.\")\n",
    "        \n",
    "        # Set random seed if provided\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Sample from expanded data\n",
    "        expanded_data = self.expansion.copy()\n",
    "        \n",
    "        # Calculate sampling weights based on treatment assignment\n",
    "        expanded_data['sample_weight'] = np.where(\n",
    "            expanded_data['trial_arm'] == 0,  # Assuming 0 is control\n",
    "            1.0 / p_control,\n",
    "            1.0 / (1.0 - p_control)\n",
    "        )\n",
    "        \n",
    "        # Store back to expansion\n",
    "        self.expansion = expanded_data\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_msm(self, weight_cols, modify_weights=None):\n",
    "        \"\"\"\n",
    "        Fit a marginal structural model using a Cox proportional hazards model.\n",
    "\n",
    "        Parameters:\n",
    "        - weight_cols: List of column names for additional weights (e.g., censoring weights).\n",
    "        - modify_weights: Optional function to modify combined weights (e.g., winsorization).\n",
    "        \"\"\"\n",
    "        # Check if expanded data exists\n",
    "        if self.expansion is None:\n",
    "            raise ValueError(\"No expanded data available. Call expand_trials first.\")\n",
    "        \n",
    "        # Prepare a copy of the expanded data\n",
    "        model_data = self.expansion.copy()\n",
    "        \n",
    "        # Combine weights: start with trial weights, then multiply by additional weights\n",
    "        model_data['combined_weight'] = model_data['weight']\n",
    "        for col in weight_cols:\n",
    "            if col in model_data.columns:\n",
    "                model_data['combined_weight'] *= model_data[col]\n",
    "        \n",
    "        # Apply weight modification if provided\n",
    "        if modify_weights is not None:\n",
    "            model_data['combined_weight'] = modify_weights(model_data['combined_weight'])\n",
    "        \n",
    "        # Ensure the outcome model is set\n",
    "        if self.outcome_model is None:\n",
    "            self.set_outcome_model()\n",
    "        \n",
    "        # Define the time variable (endog), event status (status), and covariates (exog)\n",
    "        endog = model_data['survival_time']  # Time-to-event variable\n",
    "        status = model_data['event']         # Event indicator (1 if event occurred, 0 if censored)\n",
    "        exog = sm.add_constant(model_data[['trial_arm']])  # Covariates: intercept + treatment\n",
    "        \n",
    "        # Include adjustment variables if specified\n",
    "        if self.outcome_model.adjustment_vars:\n",
    "            exog = pd.concat([exog, model_data[self.outcome_model.adjustment_vars]], axis=1)\n",
    "        \n",
    "        # Fit the Cox PH model\n",
    "        model = PHReg(endog, exog, status=status, weights=model_data['combined_weight'])\n",
    "        self.outcome_model.fitted_model = model.fit()\n",
    "        \n",
    "        # Store model information\n",
    "        self.outcome_model.model_info = {\n",
    "            'model': model,\n",
    "            'vcov': self.outcome_model.fitted_model.cov_params(),\n",
    "            'exog_names': exog.columns.tolist()\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, newdata, predict_times, type=\"survival\"):\n",
    "        \"\"\"Predict outcomes based on the fitted model.\"\"\"\n",
    "        if self.outcome_model is None or not self.outcome_model.is_fitted():\n",
    "            raise ValueError(\"Outcome model not fitted. Call fit_msm first.\")\n",
    "        \n",
    "        # Prepare prediction data\n",
    "        pred_data = newdata.copy()\n",
    "        \n",
    "        # Make predictions for each treatment arm\n",
    "        results = {}\n",
    "        for arm in [0, 1]:  # Binary treatment (0=control, 1=treatment)\n",
    "            pred_data['trial_arm'] = arm\n",
    "            surv_curves = self.outcome_model.predict(pred_data, predict_times)\n",
    "            # Compute mean over individuals (axis=0) for each time point\n",
    "            mean_survival = np.mean(surv_curves['survival'], axis=0)\n",
    "            mean_lower = np.mean(surv_curves['lower'], axis=0)\n",
    "            mean_upper = np.mean(surv_curves['upper'], axis=0)\n",
    "            results[f'arm_{arm}'] = {\n",
    "                'times': surv_curves['times'],\n",
    "                'survival': mean_survival,\n",
    "                'lower': mean_lower,\n",
    "                'upper': mean_upper\n",
    "            }\n",
    "        \n",
    "        # Calculate difference (treatment effect)\n",
    "        diff_data = pd.DataFrame({\n",
    "            'followup_time': predict_times,\n",
    "            'survival_diff': results['arm_1']['survival'] - results['arm_0']['survival'],\n",
    "            '2.5%': results['arm_1']['lower'] - results['arm_0']['upper'],  # Simplified CI\n",
    "            '97.5%': results['arm_1']['upper'] - results['arm_0']['lower']  # Simplified CI\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'arm_0': results['arm_0'],\n",
    "            'arm_1': results['arm_1'],\n",
    "            'difference': diff_data\n",
    "        }\n",
    "    \n",
    "    def _formula_to_vars(self, formula):\n",
    "        \"\"\"Convert an R-style formula to a list of variable names.\"\"\"\n",
    "        if isinstance(formula, str):\n",
    "            # Remove ~ and split by +\n",
    "            parts = formula.replace(\"~\", \"\").split(\"+\")\n",
    "            return [part.strip() for part in parts]\n",
    "        else:\n",
    "            # For our example, we'll handle the simpler case where the formula is a string like \"~ age + x1 + x3\"\n",
    "            # For the case where formula is passed as a tilde object in R, we just extract the string after the tilde\n",
    "            formula_str = formula.replace(\"~\", \"\")\n",
    "            parts = formula_str.split(\"+\")\n",
    "            return [part.strip() for part in parts]\n",
    "\n",
    "\n",
    "class StatsGlmLogit:\n",
    "    def __init__(self, save_path=None):\n",
    "        self.save_path = save_path\n",
    "        if save_path and not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "    \n",
    "    def fit(self, data, treatment_col, numerator_vars, denominator_vars, id_col, period_col):\n",
    "        \"\"\"Fit logistic regression models and calculate stabilized weights.\"\"\"\n",
    "        # Get periods where treatment can switch\n",
    "        periods = sorted(data[period_col].unique())\n",
    "        \n",
    "        # Initialize DataFrame to store weights\n",
    "        weights_df = pd.DataFrame()\n",
    "        \n",
    "        for period in periods[1:]:  # Skip the first period as there's no prior treatment to switch from\n",
    "            # Get data for current period\n",
    "            period_data = data[data[period_col] == period].copy()\n",
    "            \n",
    "            if len(period_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get previous period for each individual\n",
    "            prev_period = periods[periods.index(period) - 1]\n",
    "            prev_data = data[data[period_col] == prev_period].copy()\n",
    "            \n",
    "            # Merge current with previous period data\n",
    "            merged_data = pd.merge(\n",
    "                period_data,\n",
    "                prev_data[[id_col, treatment_col]],\n",
    "                on=id_col,\n",
    "                suffixes=('', '_prev')\n",
    "            )\n",
    "            \n",
    "            # Identify individuals who switched treatment\n",
    "            merged_data['switched'] = (merged_data[treatment_col] != merged_data[f\"{treatment_col}_prev\"]).astype(int)\n",
    "            \n",
    "            # Fit numerator model (simpler model)\n",
    "            X_num = sm.add_constant(merged_data[numerator_vars])\n",
    "            num_model = sm.Logit(merged_data['switched'], X_num).fit(disp=0)\n",
    "            \n",
    "            # Save model if requested\n",
    "            if self.save_path:\n",
    "                with open(os.path.join(self.save_path, f\"num_model_period_{period}.pkl\"), 'wb') as f:\n",
    "                    pickle.dump(num_model, f)\n",
    "            \n",
    "            # Fit denominator model (full model)\n",
    "            X_denom = sm.add_constant(merged_data[denominator_vars])\n",
    "            denom_model = sm.Logit(merged_data['switched'], X_denom).fit(disp=0)\n",
    "            \n",
    "            # Save model if requested\n",
    "            if self.save_path:\n",
    "                with open(os.path.join(self.save_path, f\"denom_model_period_{period}.pkl\"), 'wb') as f:\n",
    "                    pickle.dump(denom_model, f)\n",
    "            \n",
    "            # Calculate predicted probabilities\n",
    "            num_probs = num_model.predict(X_num)\n",
    "            denom_probs = denom_model.predict(X_denom)\n",
    "            \n",
    "            # Calculate stabilized weights\n",
    "            merged_data['weight'] = num_probs / denom_probs\n",
    "            merged_data['weight'] = merged_data['weight'].fillna(1.0)  # Handle division by zero\n",
    "            \n",
    "            # Add to weights DataFrame\n",
    "            weights_df = pd.concat([weights_df, merged_data[[id_col, period_col, 'weight']]])\n",
    "        \n",
    "        return weights_df\n",
    "\n",
    "\n",
    "class CensorWeightCalculator:\n",
    "    def __init__(self, model_fitter, censor_event, pool_models=\"none\"):\n",
    "        self.model_fitter = model_fitter\n",
    "        self.censor_event = censor_event\n",
    "        self.pool_models = pool_models  # \"none\", \"numerator\", or \"denominator\"\n",
    "    \n",
    "    def fit(self, data, numerator_vars, denominator_vars, id_col, period_col):\n",
    "        \"\"\"Calculate censoring weights.\"\"\"\n",
    "        # Create a copy of the data\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Create pooled models if required\n",
    "        if self.pool_models in [\"numerator\", \"both\"]:\n",
    "            pooled_num_model = self._fit_pooled_model(data_copy, numerator_vars)\n",
    "        \n",
    "        if self.pool_models in [\"denominator\", \"both\"]:\n",
    "            pooled_denom_model = self._fit_pooled_model(data_copy, denominator_vars)\n",
    "        \n",
    "        # Get periods\n",
    "        periods = sorted(data_copy[period_col].unique())\n",
    "        \n",
    "        # Initialize DataFrame to store weights\n",
    "        weights_df = pd.DataFrame()\n",
    "        \n",
    "        for period in periods:\n",
    "            # Get data for current period\n",
    "            period_data = data_copy[data_copy[period_col] == period].copy()\n",
    "            \n",
    "            if len(period_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Define censoring indicator\n",
    "            period_data['is_censored'] = (period_data[self.censor_event] == 1).astype(int)\n",
    "            \n",
    "            # Check if we have variation in the outcome\n",
    "            if period_data['is_censored'].nunique() <= 1:\n",
    "                # No variation, assign weight of 1.0\n",
    "                period_data['weight'] = 1.0\n",
    "            else:\n",
    "                try:\n",
    "                    # Fit period-specific models or use pooled models\n",
    "                    if self.pool_models != \"numerator\":\n",
    "                        # Fit numerator model for this period with regularization\n",
    "                        X_num = sm.add_constant(period_data[numerator_vars])\n",
    "                        num_model = sm.Logit(period_data['is_censored'], X_num).fit_regularized(\n",
    "                            alpha=0.01, disp=0\n",
    "                        )\n",
    "                        num_probs = num_model.predict(X_num)\n",
    "                    else:\n",
    "                        # Use pooled numerator model\n",
    "                        X_num = sm.add_constant(period_data[numerator_vars])\n",
    "                        num_probs = pooled_num_model.predict(X_num)\n",
    "                    \n",
    "                    if self.pool_models != \"denominator\":\n",
    "                        # Fit denominator model for this period with regularization\n",
    "                        X_denom = sm.add_constant(period_data[denominator_vars])\n",
    "                        denom_model = sm.Logit(period_data['is_censored'], X_denom).fit_regularized(\n",
    "                            alpha=0.01, disp=0\n",
    "                        )\n",
    "                        denom_probs = denom_model.predict(X_denom)\n",
    "                    else:\n",
    "                        # Use pooled denominator model\n",
    "                        X_denom = sm.add_constant(period_data[denominator_vars])\n",
    "                        denom_probs = pooled_denom_model.predict(X_denom)\n",
    "                    \n",
    "                    # Ensure probabilities are not exactly 0 or 1\n",
    "                    num_probs = np.clip(num_probs, 0.001, 0.999)\n",
    "                    denom_probs = np.clip(denom_probs, 0.001, 0.999)\n",
    "                    \n",
    "                    # Calculate stabilized weights\n",
    "                    period_data['weight'] = (1 - num_probs) / (1 - denom_probs)\n",
    "                    \n",
    "                except (np.linalg.LinAlgError, ValueError) as e:\n",
    "                    print(f\"Warning: Model fitting failed for period {period}. Setting weights to 1.0. Error: {e}\")\n",
    "                    period_data['weight'] = 1.0\n",
    "                \n",
    "            # Handle any remaining NaNs or infinities\n",
    "            period_data['weight'] = period_data['weight'].fillna(1.0)\n",
    "            period_data.loc[np.isinf(period_data['weight']), 'weight'] = 1.0\n",
    "            \n",
    "            # Trim extreme weights\n",
    "            q99 = np.percentile(period_data['weight'], 99)\n",
    "            period_data.loc[period_data['weight'] > q99, 'weight'] = q99\n",
    "            \n",
    "            # Add to weights DataFrame\n",
    "            weights_df = pd.concat([weights_df, period_data[[id_col, period_col, 'weight']]])\n",
    "        \n",
    "        return weights_df\n",
    "\n",
    "    def _fit_pooled_model(self, data, vars_list):\n",
    "        \"\"\"Fit a pooled model across all periods.\"\"\"\n",
    "        # Create censoring indicator\n",
    "        data['is_censored'] = (data[self.censor_event] == 1).astype(int)\n",
    "        \n",
    "        # Check if we have variation in the outcome\n",
    "        if data['is_censored'].nunique() <= 1:\n",
    "            # Return a dummy model that always predicts the constant\n",
    "            constant_prob = data['is_censored'].mean()\n",
    "            class DummyModel:\n",
    "                def predict(self, X):\n",
    "                    return np.ones(len(X)) * constant_prob\n",
    "            return DummyModel()\n",
    "        \n",
    "        try:\n",
    "            # Fit model with regularization\n",
    "            X = sm.add_constant(data[vars_list])\n",
    "            model = sm.Logit(data['is_censored'], X).fit_regularized(alpha=0.01, disp=0)\n",
    "            return model\n",
    "        except (np.linalg.LinAlgError, ValueError) as e:\n",
    "            print(f\"Warning: Pooled model fitting failed. Creating dummy model. Error: {e}\")\n",
    "            # Return a dummy model\n",
    "            constant_prob = data['is_censored'].mean()\n",
    "            class DummyModel:\n",
    "                def predict(self, X):\n",
    "                    return np.ones(len(X)) * constant_prob\n",
    "            return DummyModel()\n",
    "\n",
    "\n",
    "class OutcomeModel:\n",
    "    def __init__(self, adjustment_vars=None):\n",
    "        self.adjustment_vars = adjustment_vars\n",
    "        self.fitted_model = None\n",
    "        self.model_info = None\n",
    "    \n",
    "    def fit(self, data, id_col, time_col, event_col, treatment_col, weight_col):\n",
    "        \"\"\"Fit a proportional hazards model.\"\"\"\n",
    "        model_data = data.copy()\n",
    "        \n",
    "        # Prepare formula\n",
    "        if self.adjustment_vars:\n",
    "            formula = f\"{time_col} ~ {treatment_col} + \" + \" + \".join(self.adjustment_vars)\n",
    "        else:\n",
    "            formula = f\"{time_col} ~ {treatment_col}\"\n",
    "        \n",
    "        # Fit Cox PH model\n",
    "        model = PHReg.from_formula(\n",
    "            formula,\n",
    "            data=model_data,\n",
    "            status=model_data[event_col],\n",
    "            weights=model_data[weight_col]\n",
    "        )\n",
    "        \n",
    "        result = model.fit()\n",
    "        \n",
    "        # Store fitted model\n",
    "        self.fitted_model = result\n",
    "        \n",
    "        # Store model info\n",
    "        self.model_info = {\n",
    "            'model': model,\n",
    "            'vcov': result.cov_params(),\n",
    "            'formula': formula\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def is_fitted(self):\n",
    "        \"\"\"Check if model is fitted.\"\"\"\n",
    "        return self.fitted_model is not None\n",
    "    \n",
    "    def predict(self, data, times):\n",
    "        \"\"\"Predict survival probabilities.\"\"\"\n",
    "        if not self.is_fitted():\n",
    "            raise ValueError(\"Model not fitted yet.\")\n",
    "        \n",
    "        # Prepare prediction data\n",
    "        pred_data = data.copy()\n",
    "        \n",
    "        # Get baseline survival (shape: (11,))\n",
    "        baseline_surv = self._estimate_baseline_survival(times)\n",
    "        \n",
    "        # Get linear predictor (shape: (32,))\n",
    "        lp = self._calculate_linear_predictor(pred_data)\n",
    "        \n",
    "        # Calculate survival probabilities (shape: (32, 11))\n",
    "        survival = np.power(baseline_surv, np.exp(lp)[:, np.newaxis])\n",
    "        \n",
    "        # Calculate confidence intervals (simplified)\n",
    "        ci_width = 1.96 * 0.1 * survival  # Simplified CI\n",
    "        \n",
    "        return {\n",
    "            'times': times,\n",
    "            'survival': survival,\n",
    "            'lower': np.maximum(0, survival - ci_width),\n",
    "            'upper': np.minimum(1, survival + ci_width)\n",
    "        }\n",
    "    \n",
    "    def _estimate_baseline_survival(self, times):\n",
    "        \"\"\"Estimate baseline survival function.\"\"\"\n",
    "        # This is a simplified implementation\n",
    "        # A proper implementation would estimate the baseline hazard from the data\n",
    "        \n",
    "        # For simplicity, we'll use an exponential model\n",
    "        # In real implementation, this would be based on the fitted model's baseline hazard\n",
    "        lambda_hat = 0.1  # Placeholder hazard rate\n",
    "        return np.exp(-lambda_hat * np.array(times))\n",
    "    \n",
    "    def _calculate_linear_predictor(self, data):\n",
    "        exog_names = self.fitted_model.model.exog_names  # ['Intercept', 'trial_arm', 'x2']\n",
    "        X = pd.DataFrame(index=data.index)\n",
    "        for name in exog_names:\n",
    "            if name == 'Intercept':\n",
    "                X[name] = 1\n",
    "            else:\n",
    "                X[name] = data[name]\n",
    "        return X.values @ self.fitted_model.params\n",
    "\n",
    "\n",
    "def trial_sequence(estimand):\n",
    "    \"\"\"Create a new trial sequence object.\"\"\"\n",
    "    return TrialSequence(estimand=estimand)\n",
    "\n",
    "\n",
    "def stats_glm_logit(save_path=None):\n",
    "    \"\"\"Create a logistic regression model fitter.\"\"\"\n",
    "    return StatsGlmLogit(save_path=save_path)\n",
    "\n",
    "\n",
    "def save_to_datatable():\n",
    "    \"\"\"Handler for saving expanded data to a data table.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    def handler(data):\n",
    "        return data\n",
    "    return handler\n",
    "\n",
    "\n",
    "def outcome_data(trial):\n",
    "    \"\"\"Extract outcome data from a trial sequence.\"\"\"\n",
    "    if trial.expansion is not None:\n",
    "        return trial.expansion\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def show_weight_models(trial):\n",
    "    \"\"\"Display information about weight models.\"\"\"\n",
    "    print(\"Weight Models Summary:\")\n",
    "    if hasattr(trial, 'switch_weights') and trial.switch_weights is not None:\n",
    "        print(f\"  Switch Weights: {len(trial.switch_weights)} rows\")\n",
    "    \n",
    "    if hasattr(trial, 'censor_weights') and trial.censor_weights is not None:\n",
    "        print(f\"  Censor Weights: {len(trial.censor_weights)} rows\")\n",
    "    \n",
    "    if hasattr(trial, 'combined_weights') and trial.combined_weights is not None:\n",
    "        print(f\"  Combined Weights: {len(trial.combined_weights)} rows\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    data_censored = pd.read_csv(\"data/data_censored.csv\")\n",
    "    \n",
    "    # Create trial sequence objects\n",
    "    trial_pp = trial_sequence(estimand=\"PP\")  # Per-protocol\n",
    "    trial_itt = trial_sequence(estimand=\"ITT\")  # Intention-to-treat\n",
    "    \n",
    "    # Create directories\n",
    "    trial_pp_dir = os.path.join(tempfile.gettempdir(), \"trial_pp\")\n",
    "    os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "    \n",
    "    trial_itt_dir = os.path.join(tempfile.gettempdir(), \"trial_itt\")\n",
    "    os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "    \n",
    "    # Set data for both trial sequences\n",
    "    trial_pp = trial_pp.set_data(\n",
    "        data=data_censored,\n",
    "        id=\"id\",\n",
    "        period=\"period\",\n",
    "        treatment=\"treatment\",\n",
    "        outcome=\"outcome\",\n",
    "        eligible=\"eligible\"\n",
    "    )\n",
    "    \n",
    "    trial_itt = trial_itt.set_data(\n",
    "        data=data_censored,\n",
    "        id=\"id\",\n",
    "        period=\"period\",\n",
    "        treatment=\"treatment\",\n",
    "        outcome=\"outcome\",\n",
    "        eligible=\"eligible\"\n",
    "    )\n",
    "    \n",
    "    # Set switch weight models\n",
    "    trial_pp = trial_pp.set_switch_weight_model(\n",
    "        numerator=\"~ age\",\n",
    "        denominator=\"~ age + x1 + x3\",\n",
    "        model_fitter=stats_glm_logit(save_path=os.path.join(trial_pp_dir, \"switch_models\"))\n",
    "    )\n",
    "    \n",
    "    print(trial_pp)\n",
    "    # Set censor weight models\n",
    "    trial_pp = trial_pp.set_censor_weight_model(\n",
    "        censor_event=\"censored\",\n",
    "        numerator=\"~ x2\",\n",
    "        denominator=\"~ x2 + x1\",\n",
    "        pool_models=\"none\",\n",
    "        model_fitter=stats_glm_logit(save_path=os.path.join(trial_pp_dir, \"switch_models\"))\n",
    "    )\n",
    "    \n",
    "    trial_itt = trial_itt.set_censor_weight_model(\n",
    "        censor_event=\"censored\",\n",
    "        numerator=\"~ x2\",\n",
    "        denominator=\"~ x2 + x1\",\n",
    "        pool_models=\"numerator\",\n",
    "        model_fitter=stats_glm_logit(save_path=os.path.join(trial_itt_dir, \"switch_models\"))\n",
    "    )\n",
    "    \n",
    "    # Calculate weights\n",
    "    trial_pp = trial_pp.calculate_weights()\n",
    "    trial_itt = trial_itt.calculate_weights()\n",
    "    \n",
    "    # Show weight models\n",
    "    show_weight_models(trial_itt)\n",
    "    show_weight_models(trial_pp)\n",
    "    \n",
    "    # Set outcome models\n",
    "    trial_pp = trial_pp.set_outcome_model()\n",
    "    trial_itt = trial_itt.set_outcome_model(adjustment_terms=\"~ x2\")\n",
    "    \n",
    "    # Set expansion options\n",
    "    trial_pp = trial_pp.set_expansion_options(\n",
    "        output=save_to_datatable(),\n",
    "        chunk_size=500\n",
    "    )\n",
    "    \n",
    "    trial_itt = trial_itt.set_expansion_options(\n",
    "        output=save_to_datatable(),\n",
    "        chunk_size=500\n",
    "    )\n",
    "    \n",
    "    # Expand trials\n",
    "    trial_pp = trial_pp.expand_trials()\n",
    "    trial_itt = trial_itt.expand_trials()\n",
    "    \n",
    "    # Load expanded data and fit MSM\n",
    "    trial_itt = trial_itt.load_expanded_data(seed=1234, p_control=0.5)\n",
    "    trial_itt = trial_itt.fit_msm(\n",
    "        weight_cols=[\"weight\", \"sample_weight\"],\n",
    "        modify_weights=lambda w: np.minimum(w, np.quantile(w, 0.99))  # Winsorization\n",
    "    )\n",
    "    \n",
    "    # Prepare the prediction data\n",
    "    prediction_data = outcome_data(trial_itt)\n",
    "    prediction_data = prediction_data[prediction_data['trial_period'] == 1]\n",
    "\n",
    "    # Add the 'const' column with value 1\n",
    "    prediction_data['const'] = 1\n",
    "\n",
    "    # Now call predict\n",
    "    preds = trial_itt.predict(\n",
    "        newdata=prediction_data,\n",
    "        predict_times=list(range(11)),  # Assuming you want predictions from 0 to 10\n",
    "        type=\"survival\"\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(preds['difference']['followup_time'], preds['difference']['survival_diff'])\n",
    "    plt.plot(preds['difference']['followup_time'], preds['difference']['2.5%'], 'r--')\n",
    "    plt.plot(preds['difference']['followup_time'], preds['difference']['97.5%'], 'r--')\n",
    "    plt.xlabel('Follow up')\n",
    "    plt.ylabel('Survival difference')\n",
    "    plt.title('Treatment Effect on Survival')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('survival_difference.png')\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-3202",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
